{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee78e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Importing Libraries and Packages required #######\n",
    "\n",
    "import pickle\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import spatial\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fafa3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading data from the file ##\n",
    "\n",
    "# Train_Data\n",
    "train_data = pd.read_csv(r'C:\\Users\\dell\\Downloads\\Clustering_Assignment\\Final Project\\atis_intents_train.csv', header=None)\n",
    "train_data.rename(columns={0:'Label', 1:'Text'}, inplace=True)\n",
    "\n",
    "# Test Data\n",
    "test_data = pd.read_csv(r'C:\\Users\\dell\\Downloads\\Clustering_Assignment\\Final Project\\atis_intents_test.csv',header=None)\n",
    "test_data.rename(columns={0:'Label', 1:'Text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1945e7",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35db4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Preparation of the Data #############\n",
    "\n",
    "def pre_process(data):\n",
    "    \n",
    "    # Removing stopwords and numbers \n",
    "    words = set(stopwords.words('english'))\n",
    "    data['Text']=data['Text'].apply(lambda x:' '.join([word for word in x.split()if word not in (words)]))\n",
    "    data['Text']= data['Text'].str.replace('\\d+','')\n",
    "\n",
    "    text = data['Text']\n",
    "    labels = data['Label']\n",
    "    \n",
    "    return text,labels\n",
    "\n",
    "# Function for Pre-processing of Data\n",
    "train_text, train_labels = pre_process(train_data)\n",
    "test_text, test_labels = pre_process(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d41b29c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index/Vocabulary size : 631\n",
      "Max Input Size: 25 \n",
      "Train: (4834, 25)  Test: (800, 25)\n"
     ]
    }
   ],
   "source": [
    "########### Encoding the Data #############\n",
    "    \n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(train_text)\n",
    "print('Word Index/Vocabulary size :', len(t.word_index))\n",
    "word_index = t.word_index\n",
    "\n",
    "input_length = 25\n",
    "print('Max Input Size: 25 ')\n",
    "\n",
    "## Tokenizing and creating sequences in the train data ##\n",
    "train_data_tokens = t.texts_to_sequences(train_text)\n",
    "train_input = pad_sequences(train_data_tokens, input_length)\n",
    "\n",
    "## Tokenizing and creating sequences in the test data ##\n",
    "test_data_tokens = t.texts_to_sequences(test_text)\n",
    "test_input = pad_sequences(test_data_tokens,input_length)\n",
    "\n",
    "max_vocab_size = len(word_index)+1\n",
    "print('Train:',train_input.shape,' Test:',test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5784b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Encoding the Labels #############\n",
    "\n",
    "label_transformer = preprocessing.LabelEncoder()\n",
    "label_transformer.fit(train_labels)\n",
    "\n",
    "## Storing the pickle file for encoder label ##\n",
    "with open(r'C:\\Users\\dell\\Downloads\\Clustering_Assignment\\Final Project\\le.pkl','wb') as model_pkl:\n",
    "    pickle.dump(label_transformer, model_pkl, protocol=2)\n",
    "\n",
    "train_labels = label_transformer.transform(train_labels)\n",
    "test_labels = label_transformer.transform(test_labels)\n",
    "\n",
    "train_labels = to_categorical(np.asarray(train_labels))\n",
    "test_labels = to_categorical(np.asarray(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ad4002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a Validation Set ##\n",
    "X_train, X_val, y_train, y_val  = train_test_split(train_input, train_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57e3c99",
   "metadata": {},
   "source": [
    "#### Extracting Word Embeddings and creating an embedding matrix (Glove Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02a66cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting Word Embeddings using Glove ##\n",
    "\n",
    "dim = 300\n",
    "embedded_index = dict()\n",
    "with open (r'C:\\Users\\dell\\Downloads\\Clustering_Assignment\\Final Project\\glove.42B.300d.txt\\glove.42B.300d.txt','r',encoding='utf8') as glove : \n",
    "    for line in glove:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:],dtype='float32')\n",
    "        embedded_index[word] = vector        \n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed766574",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedded_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m vector \u001b[38;5;241m=\u001b[39m embedded_index\u001b[38;5;241m.\u001b[39mget(i)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43membedded_matrix\u001b[49m[j] \u001b[38;5;241m=\u001b[39m vector\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedded_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "## Creating an Embedding Matrix ##\n",
    "embedded_mat = np.zeros((max_vocab_size, dim))\n",
    "\n",
    "for i,j in word_index.items():\n",
    "    vector = embedded_index.get(i)\n",
    "    if vector is not None:\n",
    "        embedded_matrix[j] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c9f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualizing Word Embeddings Using T-SNE ##\n",
    "def similarity(embedding):\n",
    "    return sorted(embedded_index.keys(), key=lambda x: spatial.distance.euclidean(embedded_index[x],embedding))\n",
    "\n",
    "exam_flight = similarity_check(embedded_index['flight'])[1:15]\n",
    "print(exam_flight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b14ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## t-distributed stochastic neighbor embedding ##\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "vectorized = [embedded_index[word] for words in exam_flight]\n",
    "Y = tsne.fit_transform(vectorized)\n",
    "\n",
    "## Plotting the example ##\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(Y[:, 0], Y[:, 1], 'o')\n",
    "ax.set_title('Eucledian Distance of Words Relating to \"Flight\"')\n",
    "for i, word in enumerate(example):\n",
    "     plt.annotate(word, xy=(Y[i, 0], Y[i, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f5a38",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a386165",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Model Building #############\n",
    "\n",
    "def model_data(mat, input_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_vocab_size, 300, input_length= input_length, weights=[mat],trainable=False))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='selu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='selu'))\n",
    "    model.add(Dense(8, activation='sigmoid'))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec1a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting the model on train data ##\n",
    "model=model_data(embedded_matrix, input_length)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train, epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38640a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results on Validation Set ##\n",
    "model.evaluate(X_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4edcf47",
   "metadata": {},
   "source": [
    "### Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7788d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results on Test Data ##\n",
    "def acc(y_true, y_pred):\n",
    "    return np.equal(np.argmax(y_true, axis=-1), np.argmax(y_pred, axis=-1)).mean()\n",
    "\n",
    "## Accuracy for Test Data \n",
    "predictions = model.predict(test_input)\n",
    "print('Accuracy Score (Test Data):', acc(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81280366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
